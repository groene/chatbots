{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a120a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04686fd3",
   "metadata": {},
   "source": [
    "### Init files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e131540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import csv\n",
    "import gzip\n",
    "import requests\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2102e6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "TMP_DIR = 'tmp'\n",
    "DATA_PATH = 'data'\n",
    "\n",
    "C4_URL = 'https://huggingface.co/datasets/allenai/c4/resolve/main/multilingual/c4-nl.tfrecord-{}-of-01024.json.gz'\n",
    "N_FILES = 1024\n",
    "\n",
    "if not os.path.exists(TMP_DIR):\n",
    "    os.mkdir(TMP_DIR)\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    os.mkdir(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61eb7cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = os.path.join(DATA_PATH, \"dumps_url_extracted.csv\")\n",
    "\n",
    "fieldnames = ['url', 'timestamp', 'text_length', 'included']\n",
    "csv_writer = csv.DictWriter(open(csv_path, 'w', newline=''), fieldnames=fieldnames)\n",
    "\n",
    "csv_writer.writeheader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091b5821",
   "metadata": {},
   "source": [
    "### Get GPT quality filter (can be skipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e68fa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPT_FILTER = True # set to false if you want to skip these steps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b503ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://huggingface.co/spaces/ssgrn/gpt3-quality-filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cec469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"gpt3-quality-filter\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32576c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import (CountVectorizer, TfidfTransformer, HashingVectorizer,\n",
    "                                             TfidfVectorizer)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from lr.hyperparameters import SEARCH_SPACE, RandomSearch, HyperparameterSearch\n",
    "\n",
    "def load_model(serialization_dir):\n",
    "    with open(os.path.join(serialization_dir, \"best_hyperparameters.json\"), 'r') as f:\n",
    "        hyperparameters = json.load(f)\n",
    "    if hyperparameters.pop('stopwords') == 1:\n",
    "        stop_words = 'english'\n",
    "    else:\n",
    "        stop_words = None\n",
    "    weight = hyperparameters.pop('weight')\n",
    "    if weight == 'binary':\n",
    "        binary = True\n",
    "    else:\n",
    "        binary = False\n",
    "    ngram_range = hyperparameters.pop('ngram_range')\n",
    "    ngram_range = sorted([int(x) for x in ngram_range.split()])\n",
    "    if weight == 'tf-idf':\n",
    "        vect = TfidfVectorizer(stop_words=stop_words,\n",
    "                               lowercase=True,\n",
    "                               ngram_range=ngram_range)\n",
    "    elif weight == 'hash':\n",
    "        vect = HashingVectorizer(stop_words=stop_words,lowercase=True,ngram_range=ngram_range)\n",
    "    else:\n",
    "        vect = CountVectorizer(binary=binary,\n",
    "                               stop_words=stop_words,\n",
    "                               lowercase=True,\n",
    "                               ngram_range=ngram_range)\n",
    "    if weight != \"hash\":\n",
    "        with open(os.path.join(serialization_dir, \"vocab.json\"), 'r') as f:\n",
    "            vocab = json.load(f)\n",
    "        vect.vocabulary_ = vocab\n",
    "    hyperparameters['C'] = float(hyperparameters['C'])\n",
    "    hyperparameters['tol'] = float(hyperparameters['tol'])\n",
    "    classifier = LogisticRegression(**hyperparameters)\n",
    "    if os.path.exists(os.path.join(serialization_dir, \"archive\", \"idf.npy\")):\n",
    "        vect.idf_ = np.load(os.path.join(serialization_dir,  \"archive\", \"idf.npy\"))\n",
    "    classifier.coef_ = np.load(os.path.join(serialization_dir,  \"archive\", \"coef.npy\"))\n",
    "    classifier.intercept_ = np.load(os.path.join(serialization_dir,  \"archive\", \"intercept.npy\"))\n",
    "    classifier.classes_ = np.load(os.path.join(serialization_dir,  \"archive\", \"classes.npy\"))\n",
    "    return classifier, vect\n",
    "\n",
    "def score(x, clf, vectorizer):\n",
    "    return clf.predict_proba(vectorizer.transform([x]))\n",
    "\n",
    "clf, vectorizer = load_model(\"gpt3-quality-filter/model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f756baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = \"\"\"\n",
    "The British Museum is a public museum dedicated to human history, art and culture located in the Bloomsbury area of London. Its permanent collection of eight million works is among the largest and most comprehensive in existence.[3] It documents the story of human culture from its beginnings to the present.[a] The British Museum was the first public national museum to cover all fields of knowledge.[4]\n",
    "\n",
    "In 2022 the museum received 4,097,253 visitors, an increase of 209 per cent from 2021. It ranked third in the list of most-visited art museums in the world.[5]\n",
    "\n",
    "The museum was established in 1753, largely based on the collections of the Anglo-Irish physician and scientist Sir Hans Sloane.[6] It first opened to the public in 1759, in Montagu House, on the site of the current building. The museum's expansion over the following 250 years was largely a result of British colonisation and resulted in the creation of several branch institutions, or independent spin-offs, the first being the Natural History Museum in 1881. The right to ownership of some of its most well-known acquisitions, notably the Greek Elgin Marbles and the Egyptian Rosetta Stone, is subject to long-term disputes and repatriation claims.[7][8]\n",
    "\n",
    "In 1973, the British Library Act 1972[9] detached the library department from the British Museum, but it continued to host the now separated British Library in the same Reading Room and building as the museum until 1997. The museum is a non-departmental public body sponsored by the Department for Digital, Culture, Media and Sport, and as with all national museums in the UK it charges no admission fee, except for loan exhibitions.[10]\n",
    "\"\"\"\n",
    "\n",
    "pred = score(content, clf, vectorizer)\n",
    "\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b345871",
   "metadata": {},
   "source": [
    "### Download the dumps one by one, get the urls, classify them using GPT-3's quality filter (optional) and write them to a CSV (takes multiple hours) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d27dae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "\n",
    "for num in tqdm(range(start, N_FILES + 1)):\n",
    "    num_str = str(num).zfill(5)\n",
    "    url = C4_URL.format(num_str)\n",
    "    \n",
    "    tmp_path = '{}/tmp.json.gz'.format(TMP_DIR)\n",
    "    response = requests.get(url, stream=True)\n",
    "\n",
    "    with open(tmp_path, 'wb') as f:\n",
    "        for chunk in response.iter_content(chunk_size=1024):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "\n",
    "    with gzip.open(tmp_path, 'rb') as f:\n",
    "        with io.TextIOWrapper(f, encoding='utf-8') as text_file:\n",
    "            for line in text_file:\n",
    "                read_line = json.loads(line)\n",
    "                content = read_line['text']\n",
    "                \n",
    "                if USE_GPT_FILTER:\n",
    "                    pred = score(content, clf, vectorizer)\n",
    "                    quality_prob = pred[0][1]\n",
    "                    if np.random.pareto(9) > 1 - quality_prob: # Params from Brown et al. (2020), p. 43\n",
    "                        included = 1\n",
    "                    else:\n",
    "                        included = 0\n",
    "                else:\n",
    "                    included = 0\n",
    "                csv_writer.writerow({\n",
    "                    'url': read_line['url'],\n",
    "                    'timestamp': read_line['timestamp'],\n",
    "                    'text_length': len(read_line['text'].split()),\n",
    "                    'included': included\n",
    "                })\n",
    "                \n",
    "    os.remove(tmp_path)\n",
    "os.remove(tmp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1102909",
   "metadata": {},
   "source": [
    "### Read CSV in chunks and get netlocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4670395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "urldict = dict()\n",
    "urldict_quality = dict()\n",
    "\n",
    "chunk_size = 10000\n",
    "csv_reader = pd.read_csv(csv_path, chunksize=chunk_size)\n",
    "\n",
    "for chunk in tqdm(csv_reader, total = 9622):\n",
    "    netloc = chunk.url.apply(lambda x: urlparse(x).netloc)\n",
    "    for word_counts, included, netloc in zip(chunk.text_length, chunk.included, netloc):\n",
    "        if netloc not in urldict:\n",
    "            urldict[netloc] = word_counts\n",
    "        else:\n",
    "            urldict[netloc] += word_counts\n",
    "        \n",
    "        if included == 1:\n",
    "            if netloc not in urldict_quality:\n",
    "                urldict_quality[netloc] = word_counts\n",
    "            else:\n",
    "                urldict_quality[netloc] += word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702ea483",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_count_path = os.path.join(DATA_PATH, 'url_counts.csv')\n",
    "url_counts = pd.DataFrame(urldict.items(), columns=['url', 'tokens'])\n",
    "url_counts.sort_values('tokens', ascending=False).reset_index(drop=True)\n",
    "\n",
    "quality_url_count_path = os.path.join(DATA_PATH, 'quality_url_counts.csv')\n",
    "quality_url_counts = pd.DataFrame(urldict_quality.items(), columns=['url', 'tokens'])\n",
    "quality_url_counts.sort_values('tokens',\n",
    "                               ascending=False).reset_index(drop=True)\n",
    "\n",
    "# url_counts.to_csv(url_count_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a11d65",
   "metadata": {},
   "source": [
    "### Clean the data, add the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f2f4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_url(url):\n",
    "    \"\"\"\n",
    "    Remove www. prefix and 'm.' prefixes\n",
    "    \"\"\"\n",
    "    url = url.lower().strip()\n",
    "    url = url.replace('www.', '')\n",
    "    if url[:2] == 'm.':\n",
    "        url = url[2:]\n",
    "    elif '.m.' in url:\n",
    "        url = url.replace('.m.', '.')\n",
    "    return url\n",
    "\n",
    "website_labels_path = os.path.join(DATA_PATH, 'website_labels.csv')\n",
    "website_labels = pd.read_csv(website_labels_path, header=None)\n",
    "website_labels.columns = ['url', 'label']\n",
    "website_labels['url'] = website_labels.url.progress_apply(lambda x: clean_url(x[:-2]))\n",
    "\n",
    "website_labels = website_labels.drop_duplicates(keep='last')\n",
    "url2label = dict(zip(website_labels.url, website_labels.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3d116f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_counts['url'] = url_counts.url.apply(clean_url)\n",
    "quality_url_counts['url'] = quality_url_counts.url.apply(clean_url)\n",
    "\n",
    "distinct_counts = url_counts.groupby('url').sum().reset_index().sort_values('tokens', ascending=False).reset_index(drop=True)\n",
    "distinct_quality_url_counts = quality_url_counts.groupby('url').sum().reset_index().sort_values('tokens', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7563e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_counts['Rank'] = [x+1 for x in range(len(distinct_counts))]\n",
    "distinct_quality_url_counts['Rank'] = [x+1 for x in range(len(distinct_quality_url_counts))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd778c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "url2rank = dict(zip(distinct_counts.url, distinct_counts.Rank))\n",
    "\n",
    "diff = []\n",
    "for row in distinct_quality_url_counts.itertuples():\n",
    "    diff.append(url2rank[row.url] - row.Rank)\n",
    "    \n",
    "distinct_quality_url_counts['Diff'] = diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1833a5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_round(num):\n",
    "    rounded = round(num, 1)\n",
    "    counter = 1\n",
    "    if rounded == 0.0:\n",
    "        counter = 2\n",
    "        while rounded == 0.0:\n",
    "            rounded = round(num, counter)\n",
    "            counter += 1\n",
    "    str_num = f\"{rounded:.{counter}f}\"\n",
    "    if \".\" in str_num:\n",
    "        str_num = str_num.rstrip('0').rstrip('.')\n",
    "    return str_num + '%'\n",
    "\n",
    "\n",
    "distinct_counts['Label'] = distinct_counts['url'].apply(lambda x: url2label[x] if x in url2label else '').fillna('')\n",
    "distinct_quality_url_counts['Label'] = distinct_quality_url_counts['url'].apply(lambda x: url2label[x] if x in url2label else '').fillna('')\n",
    "\n",
    "distinct_counts['Aandeel'] = (distinct_counts.tokens / sum(distinct_counts.tokens) * 100).apply(custom_round)\n",
    "distinct_quality_url_counts['Aandeel'] = (distinct_quality_url_counts.tokens \n",
    "                                          / sum(distinct_quality_url_counts.tokens) * 100).apply(custom_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d914002d",
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_counts.columns = ['Url', 'Aantal woorden', 'Rank', 'Label', 'Aandeel']\n",
    "distinct_quality_url_counts.columns = ['Url', 'Aantal woorden', 'Rank', 'Verschil', 'Label','Aandeel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3b16c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_counts.to_csv(url_count_path, index=False)\n",
    "distinct_quality_url_counts.to_csv(quality_url_count_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
